---
title:  "Evaluating Election Forecasts"
subtitle: ""
excerpt: "Let's look at how the forecasters did for the 2016 Presidential election"
date: 2016-11-08
author: "Jake Thompson"
draft: false
categories:
  - R
tags:
  - politics
layout: single
---

```{r setup, include = FALSE, message = FALSE}
library(knitr)
library(MASS)
library(dplyr)
library(purrr)
library(ggplot2)
library(readxl)
library(here)

knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  echo = TRUE,
  cache = TRUE,
  fig.align = "center",
  warning = FALSE,
  message = FALSE
)
```

After more than a year and a half, the 2016 presidential election is finally
over, with Donald Trump [projected to win](http://www.nytimes.com/elections/results/president). This is in contrast to many of the election forecasts, which almost unanimously
predicted a [victory for Hillary Clinton](http://www.nytimes.com/interactive/2016/upshot/presidential-polls-forecast.html).
Now that the results are in, we can finally answer the question of who did the
best (or least worst?) job of forecasting the election.

There are several ways we could look at this question, but for the purpose of
this analysis we'll focus on two. The first is prediciton accuracy: how close
were a model's probabilities of winning each state to what actually happened? The
second is model accuracy: if we assume that the model estimated probabilities
were correct, how likely is it that we would see the outcome that was observed?

Now, let's see how some different forecasts performed.


## The Forecasts

```{r data, include = FALSE, message = FALSE}
election <- read_excel("data/election_forecasts_2016.xlsx")
colnames(election) <- c("state", "abbr", "ev", "new_york_times",
  "fivethirtyeight", "huffington_post", "predictwise",
  "princeton_electoral_consortium", "poll_savvy", "daily_kos", "crosstab",
  "slate", "map_2012", "dem_win")
```


For this analysis I'm going to focus on some of the more popular forecasts from
this election cycle: the [New York Times](http://www.nytimes.com/interactive/2016/upshot/presidential-polls-forecast.html), [FiveThirtyEight](http://projects.fivethirtyeight.com/2016-election-forecast/?ex_cid=rrpromo), [Huffington Post](http://elections.huffingtonpost.com/2016/forecast/president), [PredictWise](http://predictwise.com/), the [Princeton Election Consortium](http://election.princeton.edu/2012/09/29/the-short-term-presidential-predictor/), [Poll Savvy](https://twitter.com/PollSavvy), [Daily Kos](https://elections.dailykos.com/app/elections/2016), [Crosstab](http://www.thecrosstab.com/2016/11/07/final-2016-forecast/), and [Slate (Pierre-Antoine Kremp)](http://www.slate.com/features/pkremp_forecast/report.html).
Additionally, I'll include the 2012 electoral map as a baseline. A special thanks
to [Danny Page](http://dannypage.github.io/) who compiled all of the [data](https://docs.google.com/spreadsheets/d/1WZdg3fcvK_J-XRtN8WpEb9nRB9nF-gi6DRDYINku_PU/edit#gid=0), and whose analysis on the different forecasts can be found
[here](https://twitter.com/dannypage). As always, the code and data for this
post can be found on [my github](https://github.com/wjakethompson/wjakethompson.com).


## Prediction Accuracy

For the first part of the analysis, we'll look at how close the predictions were
in each state for each model. To do this, we'll use the [Brier Score](https://en.wikipedia.org/wiki/Brier_score), which is a measure of how
close predicted probabilities were to an observed event. The formula for the
Brier Score is given as

$$
BS = \frac{1}{N} \displaystyle\sum_{t=1}^{N}(f_t-o_t)^2
$$

where "N" is the total number of predictions, "f" is the predicted probability
of the event, and "o" is the observed outcome. Thus, a perfect prediction (e.g.,
a probability of 1.0 and the event actually occuring, or a probability of 0.0
and the event not occuring) would result in a Brier Score of 0, and the worst
possible score is 1. For example if a model predicted Hillary Clinton had an 87
percent chance of winning a state, and she ended up winning that state, that
part of the Brier score would be calculated as (0.87 - 1)^2 = 0.0169.
Because the probability is close to 1, and the event happened, the Brier Score
is low, indicating high prediction accuracy. In contrast, if Clinton were to
lose that state, the Brier score would be (0.87 - 0)^2 = 0.7569, a
relatively large number indicating a bad prediction. For any given model then,
we can add up these prediction errors for each state, and then divide by the
total number of predictions to get an overall Brier Score for each model:

```{r brier_score, echo = FALSE, strip.white = FALSE}
brier_score <- function(probs, outcome) {
  probs <- probs[which(!is.na(outcome))]
  outcome <- outcome[which(!is.na(outcome))]
  
  
  ((probs - outcome)^2) %>%
    mean()
}

forecast_lookup <- data_frame(
  rname = c("new_york_times", "fivethirtyeight", "huffington_post",
    "predictwise", "princeton_electoral_consortium", "poll_savvy",
    "daily_kos", "crosstab", "slate", "map_2012"),
  name = c("New York Times", "FiveThirtyEight", "Huffington Post",
    "PredictWise", "Princeton", "Poll Savvy",
    "Daily Kos", "Crosstab", "Slate", "2012 Map")
)

predictions <- election %>%
  select(new_york_times, fivethirtyeight, huffington_post, predictwise,
    princeton_electoral_consortium, poll_savvy, daily_kos, crosstab, slate,
    map_2012) %>%
  as.list()

scores <- pmap_dbl(.l = list(probs = predictions), .f = brier_score,
  outcome = election$dem_win)

output <- data_frame(
  method = names(scores),
  brier_score = scores
) %>%
  left_join(forecast_lookup, by = c("method" = "rname")) %>%
  arrange(brier_score) %>%
  mutate(brier_score = sprintf("%0.3f", brier_score)) %>%
  select(Forecast = name, `Brier Score` = brier_score)

knitr::kable(output)
```


Using this method, all models out perform the 2012 map, as we would expect. 
Nate Silver and FiveThirtyEight come out on top, with the lowest average
prediction error. However, we can also look at a weighted Brier Score. Maybe we
care more about an error if there were a large number of electoral votes were
at stake. For example, predicting Florida incorrectly has more implications for
who wins the presidency than predicting North Dakota incorrectly. We can add a
weight to the Brier Score formula by multiplying the electoral votes by the
error.

$$
BS = \displaystyle\sum_{t=1}^{N}EV_t(f_t-o_t)^2
$$

For this method, we are just summing all of the weighted prediction errors,
rather than taking the average. This means that the Brier score is equivalent to
the number of electoral votes that were incorrectly predicted. Thus, a perfect
score is still 0, but the worst possible score is 538, if every state were
incorrectly predicted with a probability of 0 or 1.

Using the weighted Brier Score, we see similar results.

```{r weighted_brier, echo = FALSE, strip.white = FALSE}
weighted_brier_score <- function(probs, outcome, weights) {
  if (missing(weights)) {
    weights <- rep(1, length(outcome))
  } 

  probs <- probs[which(!is.na(outcome))]
  weights <- weights[which(!is.na(outcome))]
  outcome <- outcome[which(!is.na(outcome))]
  
  
  (weights * ((probs - outcome)^2)) %>%
    sum()
}

forecast_lookup <- data_frame(
  rname = c("new_york_times", "fivethirtyeight", "huffington_post",
    "predictwise", "princeton_electoral_consortium", "poll_savvy",
    "daily_kos", "crosstab", "slate", "map_2012"),
  name = c("New York Times", "FiveThirtyEight", "Huffington Post",
    "PredictWise", "Princeton", "Poll Savvy",
    "Daily Kos", "Crosstab", "Slate", "2012 Map")
)

predictions <- election %>%
  select(new_york_times, fivethirtyeight, huffington_post, predictwise,
    princeton_electoral_consortium, poll_savvy, daily_kos, crosstab, slate,
    map_2012) %>%
  as.list()

scores <- pmap_dbl(.l = list(probs = predictions), .f = brier_score,
  outcome = election$dem_win)
weighted_scores <- pmap_dbl(.l = list(probs = predictions), .f = weighted_brier_score,
  outcome = election$dem_win, weights = election$ev)

output <- data_frame(
  method = names(scores),
  brier_score = scores,
  weighted_brier = weighted_scores
) %>%
  left_join(forecast_lookup, by = c("method" = "rname")) %>%
  arrange(weighted_brier) %>%
  mutate(brier_score = sprintf("%0.3f", brier_score),
    weighted_brier = sprintf("%0.3f", weighted_brier)) %>%
  select(Forecast = name, `Brier Score` = brier_score,
    `Weighted Brier` = weighted_brier)

knitr::kable(output)
```


FiveThirtyEight again performs the best, prediting the equivalent of about 49
electoral votes incorrectly, a full 11 electoral votes better than the second
best performing model (Crosstab). Compare this to the 2012 map, which would have
predicted 100 electoral votes incorrectly. So using either the raw or weighted
Brier Score, all models performed better than our baseline of the 2012 map, and
FiveThirtyEight performed better than all other models.


## Model Accuracy

To get an idea of the model accuracy, we can simulate the election thousands of
times for each model, assuming that their estimated probabilities are the true
probabilities in each case. For each simulation we will then get a distribution
of the number of states each model should expect to predict incorrectly. We will
then compare this expected number of incorrect picks to the observed number.
A large discrepancy between the expected and observed mean that the model's
probabilites were not consistent with the observed data, indicating poor model
accuracy.

```{r sim, echo = FALSE}
set.seed(9416)
preds <- election %>%
  filter(!grepl("_", state)) %>%
  select(-(state:ev), -map_2012, -dem_win) %>%
  as.list()
wins <- election %>%
  filter(!grepl("_", state)) %>%
  select(dem_win) %>%
  flatten_dbl()

incorrect <- map_dbl(.x = preds, function(x, wins) {
    results <- case_when(
      x < 0.5 & wins == 0 ~ 1,
      x >= 0.5 & wins == 1 ~ 1,
      TRUE ~ 0
    )
    length(which(results == 0))
  }, wins = wins)

obs_incorrect <- data_frame(
  model = names(preds),
  incorrect = incorrect
)

numsim <- 5000
incorrect_preds <- list_along(preds)
for (i in seq_along(preds)) {
  cur_pred <- preds[[i]]
  pred_list <- list_along(seq_len(numsim))
  for(j in seq_len(numsim)) {
    results <- pmap_dbl(.l = list(prob = cur_pred), .f = rbinom, n = 1, size = 1)
    correct <- case_when(
      results == 0 & cur_pred < 0.5 ~ 1,
      results == 1 & cur_pred >= 0.5 ~ 1,
      TRUE ~ 0
    )
    pred_list[[j]] <- length(which(correct == 0))
  }
  incorrect_preds[[i]] <- do.call("c", pred_list)
}
names(incorrect_preds) <- names(preds)

incorrect <- map2_df(.x = incorrect_preds, .y = names(incorrect_preds),
  .f = function(x, y) {
    data_frame(model = y, incorrect_picks = x)
  }) %>%
  left_join(obs_incorrect, by = c("model")) %>%
  left_join(forecast_lookup, by = c("model" = "rname")) %>%
  select(model = name, incorrect_picks, obs_incorrect = incorrect)
summary <- incorrect %>%
  group_by(model) %>%
  summarize(mean = mean(incorrect_picks),
    pvalue = length(which(incorrect_picks > unique(obs_incorrect))) / n()) %>%
  arrange(mean) %>%
  mutate(pvalue = sprintf("%0.3f", pvalue))

incorrect <- incorrect %>%
  mutate(model = factor(model, levels = summary$model,
    labels = paste0(summary$model, "\np = ", summary$pvalue)))
obs_incorrect <- obs_incorrect %>%
  left_join(forecast_lookup, by = c("model" = "rname")) %>%
  select(model = name, incorrect) %>%
  mutate(model = factor(model, levels = summary$model,
    labels = paste0(summary$model, "\np = ", summary$pvalue)))

ggplot() +
  facet_wrap(~ model, nrow = 3) +
  geom_histogram(data = incorrect, mapping = aes(x = incorrect_picks),
    binwidth = 1, alpha = 0.9) +
  geom_vline(data = obs_incorrect, mapping = aes(xintercept = incorrect),
    linetype = "dashed", color = "red") +
  scale_x_continuous(breaks = seq(0, 51, 2)) +
  labs(x = "Incorrectly Picked States", y = "Count") +
  theme_bw()
```


In the plot above, the red line indicates the number of states the model
actually predicted incorrectly. All of the model incorrectly predicted 5 states,
with the exception of Princeton, which incorrectly predicted only 4 states.
However, we see that we would expect most of the models to get less states
incorrect than that, given their estimated probabilities. For example, we would
expect Huffington Post to miss 1.25 states on average, and FiveThirtyEight to
miss 5.37 states on average, given their probabilities in each state. So the
Huffington Post's model was not as accurate as FiveThirtyEight's.

This occurs because Huffington Post had less uncertainty in their model, so
there were fewer states that their model thought had a chance to flip, and thus
result in an incorrect prediction. FiveThirtyEight had more uncertainty, meaning
that on average, their model would expect to get more states wrong, because
there was less certainty in who would win each state. As it turns out, this
election was a lot more uncertain than people thought, which is why
FiveThirtyEight's expected value is closer to what was observed than other
models that had less uncertainty.

We can quantify how wrong a model was by
looking at the proportion of simualtions that resulted in more incorrect picks
than what was observed (the p-value). This can be thought of as the probability
of incorrectly picking the observed number, if your probabilities were correct.
Traditionally, p-values less than 0.05 are used to indicate poor model fit. In
the graphic, we can see that only FiveThirtyEight, the New York Times, Poll
Savvy, and Princeton have p-values over 0.05. This means that we would conclude
that the probabilities provided by these models are plausible, given the data
we observed, whereas the other probabilities provided by the other models would
be rejected.

There is one problem with this simulation that we could address to make our
results more accurate. The simulation assumes that the results in each state are
indepdent of each other. However, we know this is inaccurate, as many states
tend to vote in the same way as other states with similar demographic
characteristics. Practically, this means that the distributions in the above
graphic aren't as variable as they should be. The expected value won't change
much, but the distribution of plausible numbers of incorrect picks should get
wider. Let's test that and see if our conclusions are the same.

```{r corsim, echo = FALSE}
set.seed(9416)
preds <- election %>%
  filter(!grepl("_", state)) %>%
  select(-(state:ev), -map_2012, -dem_win) %>%
  as.list()
wins <- election %>%
  filter(!grepl("_", state)) %>%
  select(dem_win) %>%
  flatten_dbl()

incorrect <- map_dbl(.x = preds, function(x, wins) {
    results <- case_when(
      x < 0.5 & wins == 0 ~ 1,
      x >= 0.5 & wins == 1 ~ 1,
      TRUE ~ 0
    )
    length(which(results == 0))
  }, wins = wins)

obs_incorrect <- data_frame(
  model = names(preds),
  incorrect = incorrect
)

dem_history <- read.csv("data/election_dem_history.csv",
  stringsAsFactors = FALSE, check.names = FALSE) %>%
  filter(Year >= 1964) %>%
  select(-Year)

rho <- cor(dem_history)

numsim <- 5000
incorrect_preds <- list_along(preds)
for (i in seq_along(preds)) {
  cur_pred <- preds[[i]]
  sigma <- rho
  for (r in 1:nrow(rho)) {
    for (c in 1:ncol(rho)) {
      varr <- (cur_pred[r]) * (1 - cur_pred[r])
      varc <- (cur_pred[c]) * (1 - cur_pred[c])
      sigma[r, c] <- rho[r, c] * sqrt(varr) * sqrt(varc)
    }
  }
  
  pred_list <- list_along(seq_len(numsim))
  for(j in seq_len(numsim)) {
    results <- mvrnorm(n = 1, mu = cur_pred, Sigma = sigma)
    results <- ifelse(results < 0.5, 0, 1)
    correct <- case_when(
      results == 0 & cur_pred < 0.5 ~ 1,
      results == 1 & cur_pred >= 0.5 ~ 1,
      TRUE ~ 0
    )
    pred_list[[j]] <- length(which(correct == 0))
  }
  incorrect_preds[[i]] <- do.call("c", pred_list)
}
names(incorrect_preds) <- names(preds)

incorrect <- map2_df(.x = incorrect_preds, .y = names(incorrect_preds),
  .f = function(x, y) {
    data_frame(model = y, incorrect_picks = x)
  }) %>%
  left_join(obs_incorrect, by = c("model")) %>%
  left_join(forecast_lookup, by = c("model" = "rname")) %>%
  select(model = name, incorrect_picks, obs_incorrect = incorrect)
summary <- incorrect %>%
  group_by(model) %>%
  summarize(mean = mean(incorrect_picks),
    pvalue = length(which(incorrect_picks > unique(obs_incorrect))) / n()) %>%
  arrange(mean) %>%
  mutate(pvalue = sprintf("%0.3f", pvalue))

incorrect <- incorrect %>%
  mutate(model = factor(model, levels = summary$model,
    labels = paste0(summary$model, "\np = ", summary$pvalue)))
obs_incorrect <- obs_incorrect %>%
  left_join(forecast_lookup, by = c("model" = "rname")) %>%
  select(model = name, incorrect) %>%
  mutate(model = factor(model, levels = summary$model,
    labels = paste0(summary$model, "\np = ", summary$pvalue)))

ggplot() +
  facet_wrap(~ model, nrow = 3) +
  geom_histogram(data = incorrect, mapping = aes(x = incorrect_picks),
    binwidth = 1, alpha = 0.9) +
  geom_vline(data = obs_incorrect, mapping = aes(xintercept = incorrect),
    linetype = "dashed", color = "red") +
  scale_x_continuous(breaks = seq(0, 51, 2)) +
  labs(x = "Incorrectly Picked States", y = "Count") +
  theme_bw()
```


As expected, the distributions are much wider once we allow for correlated
prediction errors, especially for the models that have more uncertainty in their
predictions. Using the correlated errors, Princeton, Crosstab, Poll Savvy,
the New York Times, and FiveThirtyEight all have p-values greater than 0.05,
indicating that their estimated probabilities fit our observed data.


## Conclusion

Nate Silver and FiveThirtyEight received a lot of [criticism](http://www.huffingtonpost.com/entry/nate-silver-election-forecast_us_581e1c33e4b0d9ce6fbc6f7f?ncid=engmodushpmg00000004) for being bullish on Trump, but it looks like FiveThirtyEight gets to have the last laugh. Their model outperformed all of the
other election forecasts that I analyzed: it had the best prediciton
accuracy as measured by the raw and unweighted Brier Score, and it also had the
best model accuracy, with simulations using their probabilities more closely
matching the observed data than any other model. In an incredibly messing
election, Nate Silver was the real winner.
